# SELU_Keras_Tutorial

[Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)

```
Constructing Self-Normalizing Neural Networks. (Authors) aim at constructing self-normalizing neural
networks by adjusting the properties of the function g. Only two design choices are available for
the function g: (1) the (SELU) activation function and (2) the initialization of the weights (lecun_normal).
```

![SELU HOT](https://github.com/bigsnarfdude/SELU_Keras_Tutorial/blob/master/seluSoHotRightNow.jpg)


#### Step #1

Take a look at the benchmark comparison to understand MLP-SELU

https://github.com/bigsnarfdude/SELU_Keras_Tutorial/blob/master/Basic_MLP_combined_comparison.ipynb

![SELU COMPARISON](https://github.com/bigsnarfdude/SELU_Keras_Tutorial/blob/master/sleu.png)

